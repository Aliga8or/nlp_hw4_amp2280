{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dynet as dynet\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "class Network:\n",
    "    def __init__(self, vocab, properties):\n",
    "        self.properties = properties\n",
    "        self.vocab = vocab\n",
    "\n",
    "        # first initialize a computation graph container (or model).\n",
    "        self.model = dynet.Model()\n",
    "\n",
    "        # assign the algorithm for backpropagation updates.\n",
    "        self.updater = dynet.AdamTrainer(self.model)\n",
    "\n",
    "        # create embeddings for words and tag features.\n",
    "        self.word_embedding = self.model.add_lookup_parameters((vocab.num_words(), properties.word_embed_dim))\n",
    "        self.tag_embedding = self.model.add_lookup_parameters((vocab.num_tag_feats(), properties.pos_embed_dim))\n",
    "\n",
    "        # assign transfer function\n",
    "        self.transfer = dynet.rectify  # can be dynet.logistic or dynet.tanh as well.\n",
    "\n",
    "        # define the input dimension for the embedding layer.\n",
    "        # here we assume to see two words after and before and current word (meaning 5 word embeddings)\n",
    "        # and to see the last two predicted tags (meaning two tag embeddings)\n",
    "        self.input_dim = 5 * properties.word_embed_dim + 2 * properties.pos_embed_dim\n",
    "\n",
    "        # define the hidden layer.\n",
    "        self.hidden_layer = self.model.add_parameters((properties.hidden_dim, self.input_dim))\n",
    "\n",
    "        # define the hidden layer bias term and initialize it as constant 0.2.\n",
    "        self.hidden_layer_bias = self.model.add_parameters(properties.hidden_dim, init=dynet.ConstInitializer(0.2))\n",
    "\n",
    "        # define the output weight.\n",
    "        self.output_layer = self.model.add_parameters((vocab.num_tags(), properties.hidden_dim))\n",
    "\n",
    "        # define the bias vector and initialize it as zero.\n",
    "        self.output_bias = self.model.add_parameters(vocab.num_tags(), init=dynet.ConstInitializer(0))\n",
    "\n",
    "    def build_graph(self, features):\n",
    "        # extract word and tags ids\n",
    "        word_ids = [self.vocab.word2id(word_feat) for word_feat in features[0:5]]\n",
    "        tag_ids = [self.vocab.feat_tag2id(tag_feat) for tag_feat in features[5:]]\n",
    "\n",
    "        # extract word embeddings and tag embeddings from features\n",
    "        word_embeds = [self.word_embedding[wid] for wid in word_ids]\n",
    "        tag_embeds = [self.tag_embedding[tid] for tid in tag_ids]\n",
    "\n",
    "        # concatenating all features (recall that '+' for lists is equivalent to appending two lists)\n",
    "        embedding_layer = dynet.concatenate(word_embeds + tag_embeds)\n",
    "\n",
    "        # calculating the hidden layer\n",
    "        # .expr() converts a parameter to a matrix expression in dynet (its a dynet-specific syntax).\n",
    "        hidden = self.transfer(self.hidden_layer.expr() * embedding_layer + self.hidden_layer_bias.expr())\n",
    "\n",
    "        # calculating the output layer\n",
    "        output = self.output_layer.expr() * hidden + self.output_bias.expr()\n",
    "\n",
    "        # return the output as a dynet vector (expression)\n",
    "        return output\n",
    "\n",
    "    def train(self, train_file, epochs):\n",
    "        # matplotlib config\n",
    "        loss_values = []\n",
    "        plt.ion()\n",
    "        ax = plt.gca()\n",
    "        ax.set_xlim([0, 10])\n",
    "        ax.set_ylim([0, 3])\n",
    "        plt.title(\"Loss over time\")\n",
    "        plt.xlabel(\"Minibatch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "\n",
    "        for i in range(epochs):\n",
    "            print 'started epoch', (i+1)\n",
    "            losses = []\n",
    "            train_data = open(train_file, 'r').read().strip().split('\\n')\n",
    "\n",
    "            # shuffle the training data.\n",
    "            random.shuffle(train_data)\n",
    "\n",
    "            step = 0\n",
    "            for line in train_data:\n",
    "                fields = line.strip().split('\\t')\n",
    "                features, label = fields[:-1], fields[-1]\n",
    "                gold_label = self.vocab.tag2id(label)\n",
    "                result = self.build_graph(features)\n",
    "\n",
    "                # getting loss with respect to negative log softmax function and the gold label.\n",
    "                loss = dynet.pickneglogsoftmax(result, gold_label)\n",
    "\n",
    "                # appending to the minibatch losses\n",
    "                losses.append(loss)\n",
    "                step += 1\n",
    "\n",
    "                if len(losses) >= self.properties.minibatch_size:\n",
    "                    # now we have enough loss values to get loss for minibatch\n",
    "                    minibatch_loss = dynet.esum(losses) / len(losses)\n",
    "\n",
    "                    # calling dynet to run forward computation for all minibatch items\n",
    "                    minibatch_loss.forward()\n",
    "\n",
    "                    # getting float value of the loss for current minibatch\n",
    "                    minibatch_loss_value = minibatch_loss.value()\n",
    "\n",
    "                    # printing info and plotting\n",
    "                    loss_values.append(minibatch_loss_value)\n",
    "                    if len(loss_values)%10==0:\n",
    "                        ax.set_xlim([0, len(loss_values)+10])\n",
    "                        ax.plot(loss_values)\n",
    "                        plt.draw()\n",
    "                        plt.pause(0.0001)\n",
    "                        progress = round(100 * float(step) / len(train_data), 2)\n",
    "                        print 'current minibatch loss', minibatch_loss_value, 'progress:', progress, '%'\n",
    "\n",
    "                    # calling dynet to run backpropagation\n",
    "                    minibatch_loss.backward()\n",
    "\n",
    "                    # calling dynet to change parameter values with respect to current backpropagation\n",
    "                    self.updater.update()\n",
    "\n",
    "                    # empty the loss vector\n",
    "                    losses = []\n",
    "\n",
    "                    # refresh the memory of dynet\n",
    "                    dynet.renew_cg()\n",
    "\n",
    "            # there are still some minibatch items in the memory but they are smaller than the minibatch size\n",
    "            # so we ask dynet to forget them\n",
    "            dynet.renew_cg()\n",
    "\n",
    "    def decode(self, words):\n",
    "        # first putting two start symbols\n",
    "        words = ['<s>', '<s>'] + words + ['</s>', '</s>']\n",
    "        tags = ['<s>', '<s>']\n",
    "\n",
    "        for i in range(2, len(words) - 2):\n",
    "            features = words[i - 2:i + 3] + tags[i - 2:i]\n",
    "\n",
    "            # running forward\n",
    "            output = self.build_graph(features)\n",
    "\n",
    "            # getting list value of the output\n",
    "            scores = output.npvalue()\n",
    "\n",
    "            # getting best tag\n",
    "            best_tag_id = np.argmax(scores)\n",
    "\n",
    "            # assigning the best tag\n",
    "            tags.append(self.vocab.tagid2tag_str(best_tag_id))\n",
    "\n",
    "            # refresh dynet memory (computation graph)\n",
    "            dynet.renew_cg()\n",
    "\n",
    "        return tags[2:]\n",
    "\n",
    "    def load(self, filename):\n",
    "        self.model.populate(filename)\n",
    "\n",
    "    def save(self, filename):\n",
    "        self.model.save(filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
